{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前情提要\n",
    "## 线性回归\n",
    "即便有x的高次项(多项式回归), 仍然是线性的, 因为x的高次方可以看成y, z等变量, 只不过\\theta变量的数目增多了. \n",
    "\n",
    "这个方法叫特征提取 feature extraction:\n",
    "\n",
    "通过选择不同x的组合, 变成新的特征, 进行一次幂的拟合\n",
    "\n",
    "## 逻辑斯蒂回归(其实是分类)\n",
    "\n",
    "多分类需要学完神经网络\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "\n",
    "## 引言\n",
    "\n",
    "### 逻辑斯蒂回归的局限性\n",
    "\n",
    "之前的逻辑斯蒂回归,虽然画线分类,但是可能不太好, 我们可能想要曲线\n",
    "\n",
    "有人说用逻辑斯蒂回归也可以, 用x的高次项,初等函数式, 代替x ,仍然看成线性的, 这也叫特征工程\n",
    "\n",
    "但是成本高而且效果差\n",
    "\n",
    "我们想要一种模型, 它可以自己学习\n",
    "\n",
    "### 为什么需要NN\n",
    "\n",
    "一张图片是W * H * rgb 的矩阵, 有太多的特征, 不可能手动组合\n",
    "\n",
    "## 什么是NN\n",
    "\n",
    "神经网络(现在, 计算机上的NN和生物学的神经元已经不一样了)\n",
    "\n",
    "连接主义认为, 有很多的输入, 经过了一个谁也不知道的Processing,只输出了一个东西, 这个输出传递给之后的神经元, 相当于输出成为了next input...\n",
    "\n",
    "向下 以此类推\n",
    "\n",
    "![1p](.\\1.png)\n",
    "\n",
    "这样, 就像神经元一样, 形成一个网状结构\n",
    "\n",
    "### 例如: 一个单一的逻辑神经元:\n",
    "\n",
    "![2](.\\2.png)\n",
    "\n",
    "有三个输入, 其中多出来的x_{0}是权值, \\theta_{0}是权重\n",
    "\n",
    "$$feature: x=\\begin{bmatrix}x_{0}\\\\x_{1}\\\\x_{2}\\\\x_{3}\\end{bmatrix}$$\n",
    "$$weights: \\theta=\\begin{bmatrix}\\theta_{0}\\\\\\theta_{1}\\\\\\theta_{2}\\\\\\theta_{3}\\end{bmatrix}$$\n",
    "线性叠加:\n",
    "$$h_{\\theta}(x)=\\theta_{0}x_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}+\\theta_{3}x_{3}$$\n",
    "\n",
    "人的神经元细胞中: 电平信号超过一个值才会传递给下一个神经元,\n",
    "\n",
    "因此这里也需要一个激活函数Activation function, 把线性转化成非线性\n",
    "\n",
    "比如阶跃函数, sigmoid函数, 双曲正切, relu函数(y = 0 if x<0 else kx)\n",
    "\n",
    "每次做完加权求和必须做激活\n",
    "\n",
    "### 稍微复杂一点的神经网络:\n",
    "![p3](.\\3.png)\n",
    "\n",
    "显然,从三个输入开始, 考虑再加一个权值x0, 要得到三个a, 中间必须要4*3也就是12个\\theta.\n",
    "\n",
    "由三个a得到最终结果,需要4*1个\\theta\n",
    "\n",
    "前面的x叫输入层,中间的不管多少个,都是隐藏层, 最终的是输出层\n",
    "\n",
    "相邻层之间是全连接的, 所以也叫全连接网络\n",
    "\n",
    "写成矩阵:\n",
    "$$x=\\begin{bmatrix}x_{1}\\\\x_{2}\\\\x_{3}\\end{bmatrix}$$\n",
    "增加x0\n",
    "$$x=\\begin{bmatrix}x_{0}\\\\x_{1}\\\\x_{2}\\\\x_{3}\\end{bmatrix}$$\n",
    "$$\\theta=\\begin{bmatrix}\\theta_{10}&\\theta_{11}&\\theta_{12}&\\theta_{13}\\\\\\theta_{20}&\\theta_{21}&\\theta_{22}&\\theta_{23}\\\\\\theta_{30}&\\theta_{31}&\\theta_{32}&\\theta_{33}\\end{bmatrix}$$\n",
    "$$a=\\theta \\cdot x = \\begin{bmatrix}a_{1}\\\\a_{2}\\\\a_{3}\\end{bmatrix}$$\n",
    "\n",
    "\\theta: 下一层的个数 * 上一层的个数加1 的这样一个矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AND运算\n",
    "把AND运算的逻辑简要地(不细致追究0和1的位置)画在坐标上:\n",
    "\n",
    "![p4](.\\4.png)\n",
    "\n",
    "发现它是个线性可分类模型\n",
    "\n",
    "在NN中, 通过调权重, 然后sigmoid激活就可以实现\n",
    "\n",
    "![5](.\\5.png)\n",
    "\n",
    "### OR 运算\n",
    "同理\n",
    "\n",
    "![6](.\\6.png)\n",
    "\n",
    "### Negation 取反\n",
    "\n",
    "同理\n",
    "\n",
    "![7](.\\7.png)\n",
    "\n",
    "像这种一层的神经网络叫做perceptron, 感知器\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 异或/同或运算\n",
    "\n",
    "一层网络无法完成\n",
    "\n",
    "要两层\n",
    "\n",
    "同或: a ⊙ b = (~a&~b)|(a&b) 同一异零\n",
    "\n",
    "![p8](.\\8.png)\n",
    "\n",
    "![p9](.\\9.png)\n",
    "\n",
    "异或: a ⊕ b = (a|b)&(~a|~b)  同零异一\n",
    "\n",
    "![10](.\\10.png)\n",
    "\n",
    "两层可以拟合任何复杂的模型\n",
    "\n",
    "正如离散数学中已经证明只用异或可以组合出任意的逻辑\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络在图像分类里的应用\n",
    "\n",
    "一张图有很多像素点, w 乘以 h 乘以 3 个数据\n",
    "\n",
    "它们作为输入, 行人用1(1000), 当然用one hot编码(独热编码),计算机能看懂的\n",
    "\n",
    "汽车2 (0100), 摩托3 (0010), 卡车4 (0001),\n",
    "\n",
    "但是, 分类会损失\n",
    "\n",
    "比如一个卡车图片, 本应该是4, 结果是1 ,结果差别太大, 应该惩罚这种模型\n",
    "\n",
    "如果真实值是4 ,模拟出来3, 这样不行, 说明模型对于1和3有偏见, 也就是对于4, 更倾向于预测成摩托车, 而不会预测到行人\n",
    "\n",
    "因此保证标签之间的距离应该是相等的\n",
    "\n",
    "除非有需求,设成1, 3.8 , 3.9 ,4之类的\n",
    "\n",
    "我们最终希望得到的4个数相加应该是1, 看概率最大的\n",
    "\n",
    "其实独热编码的意义就是 对应位为1 的表示概率为1, 位为0表示概率为0\n",
    "\n",
    "这种一般是真实标签\n",
    "\n",
    "真实的结果可能是0.7 , 0.1 , 0.1 , 0.1 比较合理\n",
    "\n",
    "Cost function:\n",
    "$$J(\\theta)=-\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K}\\left [y_{k}^{i} log(h_{\\theta}(x^i))_{k}+\\left (1-y_{k}^{i}log\\left (1-\\left (h_{\\theta} \\left (x_{i} \\right )\\right)_{k}\\right ) \\right ) \\right ]$$\n",
    "k: 输出的个数\n",
    "\n",
    "m: 样本的个数 即图片的张数\n",
    "\n",
    "也就是对每一张图片, 每个类别都有损失\n",
    "\n",
    "### 反向传播\n",
    "\n",
    "多个输入, 中间的所有\\theta都是参数, 得到输出\n",
    "\n",
    "输出本来是y \n",
    "\n",
    "cost function也就是J, 是y的函数, 我们把y换成J\n",
    "\n",
    "向前求$$\\frac{\\partial J}{\\partial \\theta}$$很困难, 需要反向传播求导\n",
    "\n",
    "也就是复合函数求偏导, 需要用链式法则\n",
    "\n",
    "sigmoid: \n",
    "$$f(z)=\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "$$\\frac{df(z)}{dz}=f(z)(1-f(z))$$\n",
    "\n",
    "实际工程中, 每一层的神经元可以储存这一层的梯度, 以减少内存减少\n",
    "\n",
    "BP算法(Hinton , Benjo)保证了这一点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x,y坐标中,假设有9个点, 我们想要得到一个模型\n",
    "\n",
    "9个方程,解得一个完全正确 , 不能容忍误差的模型,\n",
    "\n",
    "这种模型太复杂了, 过拟合了, 在预测新点时, 不准确\n",
    "\n",
    "因此应该大道至简(剃刀原理)\n",
    "\n",
    "对于9次的函数, x^9的影响太大, 它的系数a应该小\n",
    "\n",
    "把这些系数都放在损失函数里, 再加上一个衡量损失的函数\n",
    "\n",
    "衡量损失的函数可以是,:\n",
    "$$\\lambda \\sum |a_{i}|^n$$\n",
    "这叫正则化. n是几次幂就叫L几正则, 常用L1(Lasso 回归), L2(Ridg 岭回归)\n",
    "\n",
    "\n",
    "为了拟合不同点，overfit的函数往往需要在相对较小的区间内迅速变化，因而需要较大的梯度。为了保证较大的梯度，只能令权重较大。因而，Regularization保证了较小的权重，所以能够起到防止overfit的作用。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}