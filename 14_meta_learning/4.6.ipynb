{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 元学习\n",
    "\n",
    "> 一个古老又崭新的话题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 迁移学习\n",
    "\n",
    "\n",
    "分类:\n",
    "- inductive归纳学习\n",
    "- transductive转导学习\n",
    "\n",
    "传统机器学习都是监督学习\n",
    "\n",
    "从已有的样本,标签,找到一种映射关系\n",
    "\n",
    "\n",
    "换到另一个问题上, 两者是相互独立的(之前学的不能用)\n",
    "\n",
    "而迁移学习: 学习任务1, 获得了一些knowledge, 分享给第二个任务\n",
    "\n",
    "也就是我们不想重头学习\n",
    "\n",
    "分享什么: 权重, 最优化策略, 初始化策略等等\n",
    "\n",
    "这样会导致:\n",
    "- 第二个模型需要的数据少\n",
    "- 可能过拟合\n",
    "- 在线学习, 也就是系统每时每刻都拿到样本更新自己\n",
    "\n",
    "\n",
    "## 应用场景:\n",
    "- 德语词性标注->英语\n",
    "- 声音转文字(李宏毅的ppt不错)->YouTube的英译汉\n",
    "- 合同->网页,新闻稿\n",
    "\n",
    "在有关联但不相似的问题之间应用\n",
    "\n",
    "同一个domain就是说数据的分布类似\n",
    "\n",
    "迁移学习就是不同域之间的迁移: domain adapted领域自适应\n",
    "\n",
    "### 就图像领域来说\n",
    "\n",
    "在源数据集, 不关心最终效果如何\n",
    "\n",
    "源任务模型,比如ImageNet\n",
    "\n",
    "比如由ImageNet实物分类到卡通图像, 手绘图像分类\n",
    "\n",
    "tasks需要有共同点或相似点\n",
    "\n",
    "这时,用的更多的是参数共享\n",
    "\n",
    "\n",
    "\n",
    "训练的时候 分享的backbone 如果继续学习 它会不会忘记之前的内容 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 迁移学习定义\n",
    "需求:\n",
    "- 数据宝贵\n",
    "- 训练时间减少, \n",
    "- 可以在训练之间归纳总结\n",
    "- 有先验知识\n",
    "\n",
    "Research problem in machine learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![20](20.PNG)\n",
    "\n",
    "在源域目标域上分别有没有标签, 产生了四类问题\n",
    "\n",
    "![10](10.PNG)\n",
    "\n",
    "两个都有标签, 只关注新任务, 叫做Finetune; \n",
    "\n",
    "都想做的好, 这个叫multitask\n",
    "\n",
    "### Finetune\n",
    "\n",
    "1. 模型迁移哪一部分\n",
    "    - 冻结(直接拿来用), 对于cv的cnn来说, 往往是固定靠前的层来共享, 因为共享权值时共享的是根本性的特征. 越往前越根本, 越往后越抽象,高级,个性化\n",
    "\n",
    "2. 后来的训练好了,原来的又差了(灾难性遗忘)\n",
    "    - 加一个约束output close\n",
    "    - 两者的输出加一个L2正则, 衡量变化差异\n",
    "\n",
    "### Multitask\n",
    "\n",
    "做语义分割\n",
    "\n",
    "为了提升语义分割的准确性, 添加子任务\n",
    "- 景深估计\n",
    "- 实例分割\n",
    "\n",
    "它们的任务的Loss需要叠加在一起\n",
    "\n",
    "![30](30.PNG)\n",
    "\n",
    "中间的是共享层, 后面的是功能层\n",
    "\n",
    "##### 为了解决第一个任务遗忘 Progressive Neural Network\n",
    "\n",
    "直接把第一个任务的权值拿来用\n",
    "\n",
    "![40](40.PNG)\n",
    "\n",
    "但是次序没有规律, 可能要尝试很多次\n",
    "\n",
    "\n",
    "#### Domain-Adversarial Neural Network\n",
    "\n",
    "域迁移\n",
    "\n",
    "源数据和目标数据分布应该一致\n",
    "\n",
    "但是衡量两个数据集相似程度没有业界共识\n",
    "\n",
    "解决:\n",
    "\n",
    "对抗神经网络Domain-Adversarial Neural Network\n",
    "\n",
    "源数据, 目标数据, 经过backbone得到特征层, \n",
    "\n",
    "特征层做多个任务, 我们希望特征层的分布和原来还是一致的\n",
    "\n",
    "![50](50.PNG)\n",
    "\n",
    "图中粉色的就是domain classifier\n",
    "\n",
    "用于判断数据来自于哪个训练集的, 也就是2分类\n",
    "\n",
    "其Loss小, 说明两个数据差别不大\n",
    "\n",
    "梯度回传的时候,是上升 ,而不是下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "迁移学习可以参考[jindongwang的github](https://github.com/jindongwang)\n",
    "\n",
    "\n",
    "一个网站[transformlearning](transformlearning.xyz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 终身机器学习Lifelong Machine learning\n",
    "> 也叫增量学习, 或者永不停歇的学习\n",
    "\n",
    "## 引言\n",
    "\n",
    "为了解决AGI 通用人工智能\n",
    "\n",
    "AI人工智能是很泛化的词\n",
    "\n",
    "现在都是固定训练集, 训练解决这一个问题的模型\n",
    "\n",
    "完成不同的问题需要更换模型\n",
    "\n",
    "AGI不需要更换模型的AI, 针对一个模型\n",
    "\n",
    "就像一个小孩, 生下来大脑结构就固定了, 不断地训练他就好了\n",
    "\n",
    "针对一个模型, 给它不断地派任务, 满足多个任务\n",
    "\n",
    "## 灾难性遗忘\n",
    "\n",
    "梯度下降的更新算法, 迟早会忘记原来的任务\n",
    "\n",
    "![60](60.PNG)\n",
    "\n",
    "因为任务模型确定后, 也就是函数空间确定了, 对A最好的参数值也就是最优点确定了\\theta A\n",
    "\n",
    "但是把这个点慢慢更新到任务B上, 如果此时只关心B, 往往会遗忘A.也就是蓝色箭头\n",
    "\n",
    "我们原本希望AB都能做好, 也就是红箭头终点位置(EWC)\n",
    "\n",
    "加了L2约束, 却到了绿色箭头位置\n",
    "\n",
    "## EWC Elastic Weight Consolidation\n",
    "\n",
    "旧模型参数\\theta_{i}, 新模型 \\theta_{i}^{b}\n",
    "\n",
    "找到一种方式从\\theta_{i}更新到 \\theta_{i}^{b}\n",
    "\n",
    "使得新模型Loss'降低, 但同时保持旧模型Loss不变\n",
    "\n",
    "所以在L'加一些约束\n",
    "\n",
    "$$L'(\\theta)=L(\\theta)+\\lambda\\sum_{i}b_{i}(\\theta_{i}- \\theta_{i}^{b})^2$$\n",
    "\n",
    "其中bi是权值\n",
    "\n",
    "参数更新\n",
    "\n",
    "Loss等高线俯视图:\n",
    "![70](70.PNG)\n",
    "\n",
    "在任务1上, 从\\theta0更新到\\thetab, 放到任务二中, 倾向于向中心点更新, 但是这样久对任务1遗忘了\n",
    "\n",
    "对于任务1, 我们考虑loss对于\\theta1 和\\theta2的二阶偏导, 发现loss对\\theta1不敏感,对\\theta2敏感\n",
    "\n",
    "因此Loss中, 我们应该对\\theta2惩罚大一些, 对\\theta1惩罚小一些\n",
    "\n",
    "这样不同的参数有个不同的惩罚系数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trasnfer Learning vs. Lifelong Learning\n",
    "\n",
    "t: 我能学习task2因为我学过task1\n",
    "\n",
    "l: 尽管我能学习task2, 我没忘记task1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从ImageNet迁移到Cifar10训练\n",
    "\n",
    "[参考另一个ipynb文件](./transferlearning-vgg16-cifar10-1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 元学习 Meta-Learning\n",
    "\n",
    "> 学习如何学习 Learn to Learn\n",
    "\n",
    "## 引言\n",
    "\n",
    "什么是元: 基础, 超\n",
    "\n",
    "之前都是在针对一个任务学习\n",
    "\n",
    "现在是以任务为导向\n",
    "\n",
    "现在我们想在不同任务之间可以学习到不变的东西\n",
    "\n",
    "学习到一种技巧, 这种技巧是超脱于基础知识之外的\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简述\n",
    "\n",
    "传统ml样本xi, yi是训练集mini-batch的数据样本\n",
    "\n",
    "把这些样本看成数据集, 就是Meta Learning\n",
    "\n",
    "也就是元学习的样本是任务, 而不是数据\n",
    "\n",
    "传统ML找到映射关系f(x)=y\n",
    "\n",
    "MetaL是找到学习f的F, F(Dataset) = f也就是普适性的\n",
    "\n",
    "比如怎么选模型结构, 初始化, 怎么更新参数等等这些学习策略\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot learning\n",
    "\n",
    "Few-shot learning少样本学习(一般<5, 甚至one-shot)\n",
    "\n",
    "MetaLearning需要很大的计算能力\n",
    "\n",
    "我们用Few-shot learning来检测MetaLearning的算法正确性\n",
    "\n",
    "也就是将多个fsl的任务组成meta learning\n",
    "\n",
    "### 比如物体分类  \n",
    "- SupportSet: 5个类别, 每个类别5个图(5way, 5shot)\n",
    "- QuerySet:   任务是给新的图片, 问是什么类别\n",
    "\n",
    "元学习就是对多个这样的5way5shot任务f1, f2, f3, ...\n",
    "\n",
    "![80](80.PNG)\n",
    "\n",
    "### 又比如Omniglot\n",
    "古文字\n",
    "\n",
    "1623way 20shot\n",
    "\n",
    "shape是Mnist的转置\n",
    "\n",
    "### 再比如MiniImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAML: Model-Agnostic Meta-Learning 模型无关的元学习\n",
    "\n",
    "[论文](https://arxiv.org/abs/1703.03400)\n",
    "### 简述\n",
    "maml是为了找一个模型初始值, 需要它在不同的任务上更新都快\n",
    "\n",
    "![90](90.PNG)\n",
    "\n",
    "初始值选择,应该考虑多个任务 ,不应对某一个特别偏, 也就是一个中庸的位置\n",
    "### 过程\n",
    "1. 随机选择初值\n",
    "2. 考虑n个任务, 每个任务更新两次, 这样有2n个方向, 我们取n个任务各自第二次的更新方向, 作为初始值接下来的变化方向\n",
    "3. 不断迭代\n",
    "\n",
    "\n",
    ">为什么看两步: 因为我们需要一个初始值, 在更新进x步之后每个任务的Loss都比较好(都向着最终目标), 为了减少计算量x取1\n",
    "> ![100](100.PNG)\n",
    "> 我们要的是初始值, 应该看未来怎么样, 而不是看当下怎么样\n",
    ">\n",
    "> 正因如此, maml降低了在训练集上的表现,但是增加了在没见过的模型上的表现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reptile\n",
    "\n",
    "与maml类似, 但是, 更新的时候 ,沿着每个任务未来几步的累加向量和更新\n",
    "\n",
    "![110](110.PNG)\n",
    "\n",
    "介于maml和pretrain之间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 元学习其他方面, 除了初始化\n",
    "model-based: \n",
    "\n",
    "metric-based: 想孪生网络一样, 比较输出的差异, 主要针对few-shot\n",
    "\n",
    "optimization-based: \n",
    "\n",
    "init-based:\n",
    "\n",
    "memory-based:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitdeeplearningconda4eb25749ed314365ad1de507c177859b",
   "display_name": "Python 3.7.6 64-bit ('deeplearning': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}