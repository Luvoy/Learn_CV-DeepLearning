{"cells":[{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["# CNN Convolution Neural Network\n","\n","## 引言\n","\n","DNN：deep neural Network\n","\n","从上到下叫宽度\n","\n","左到右是深度\n","\n","perception：感知机，一层的\n","\n","Artificial neural network 就是nn\n","\n","考虑一个WxHxC的图像，维度很高，dnn的话，后面参数更多\n","\n","容易模型复杂，过拟合\n","\n","因此传统的机器视觉不用DNN\n","\n","一般提取特征，把特征作为输入，再用机器学习\n","\n","2012，CNN的AlexNet超出其他传统机器学习建模方式，imagenet图像分类比赛也火起来\n","\n","CNN是Yann LeCun在1998提出的，当时叫LeranNet5，但是和传统计算机视觉效果差别不大，主要针对10分类"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["## CNN 做了什么事\n","### 引言 卷积的实质\n","求导提取了图片的边缘信息\n","\n","一个具体滤波核其实就是一个特征\n","\n","只有跟这个核匹配好的，输出结果才比较大\n","\n","卷积过程本质上是遍历图片，输出和卷积核匹配最好的新图片\n","\n","也就是卷积核表达最剧烈的位置的输出\n","\n","所以kernel是可以学习的，kernel就是特征\n","\n","### 应用\n","- Ojbect Detection目标检测\n","    - 主要是定位\n","- 图像分隔Segmentation\n","    - 这是个分类问题，对像素点分类，比如把街景上的车涂成蓝色\n","    - 按语义分割\n","    - 按实例分割\n","    - 按目标分割\n","- 图像风格转换\n","    - 现代楼层摄影融合到梵高的星空中\n","    - 去雾霾化，变亮图片，去反光\n","- GAN\n","    - 正脸到侧脸的映射\n","- 面部关键点\n","- 人脸密度统计\n","- 给图片加描述CV +NLP\n","- 目标检测优化，比如框框旋转，选取的更好\n","- 图片有第三维深度，视频、CT、核磁共振（传统的图片的通道不是！）\n","\n","### 过程\n","\n","#### 传统机器学习\n","\n","比如输入是一张图片\n","\n","手动设计规则，比如SIFT\n","\n","提取特征\n","\n","加以选择\n","\n","属于一个类别的图片有很多张\n","\n","然后用经典机器学习工具得到目标\n","\n","结果可能不太理想TP，TN，FP，FN\n","\n","优势：\n","- 可解释性强，数学可控\n","缺点：\n","- 分离难，特征选择方式不一样，比如建筑图特征多，这样分离的成本高\n","- 结果可能不太理想\n","- 难以学习和发扬\n","\n","#### CNN过程\n","\n","优点：\n","- 端到端\n","- 便于改善，易于扩展补充\n","- 预测好\n","- 解释性差，解释像“炼丹”，比较宏观，实验推结果，而不是理论指导实践。过程像黑箱，很复杂，结果强行解释\n","\n","## CNN组成\n","\n","模型分三块\n","\n","![p1](1.png)\n","\n","- 跟数据有关的部分\n","    - Augmentation（随机）\n","    - Selection Strategy\n","        - 如果输入900张猫100张狗，那么输出可能大概率是猫，这样应该做一些采样优化，或者改一下loss\n","    \n","- 跟模型有关的部分\n","    - Initialization Strategy\n","        - 初始化参数权重w\n","        - 偏重b\n","    - 学习策略\n","        - 学习率lr\n","    - 优化策略optimization Strategy\n","        - 梯度下降\n","        - mini_batch（SGD）\n","        - 迭代多次epoch等等\n","        - 加入动量\n","    - 模型的结构\n","        - 框架framework\n","            - 模块modules\n","                - 层layers\n","- 跟loss有关的部分\n","\n","## CNN组成\n","- 卷积\n","- pooling\n","- fully connected layers:全连接层，也是一个密集层\n","- softmax\n","- cross entropy\n","\n"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["### 卷积过程\n","\n","三个通道和三个核分别做卷积，再对应element相加，得到一张图\n","\n","这种三个核叫一个KxKxC的一个核，k是边长，c是通道数\n","\n","一张图片，N个核，输出张图\n","\n","也就是由HxWxC得到H'xW'xN\n","\n","相当于得到了N张特征图，也就是得到原图对N个不同特征核的响应情况\n","\n","当k=1时，可以起到升维或者降维的作用，相当于把通道弄多了\n","\n","$$H'=\\left \\lfloor\\frac{H_{in}+2pad_{H}-dilation_{H}\\times(k_{H}-1) }{stride_{H}} + 1  \\right \\rfloor$$\n","\n","超过255的 会取余\n","\n","或者cnn的不需要给人看，做归一化处理\n","\n","![2](2.png)\n","\n","\n","带空洞dilation的卷积可以提升我们的感受野receptive field\n","\n","#### 卷积的前向传播\n","\n","结果矩阵的展开成一列向量 = C · 输入矩阵展开成一列向量\n"," \n","$$h=C\\cdot I$$\n","\n","\n","\n","可以看出卷积就是个Full connection\n","\n","w：有些权值为0， 有些权值共享\n","\n","其中C只和卷积核有关\n","\n","卷积核->C\n","\n","反卷积<-C^T\n","\n","$$\\frac{\\partial Loss}{\\partial I}= C^T \\cdot \\frac{\\partial Loss}{\\partial h}$$\n","\n","$$\\frac{\\partial Loss}{\\partial I_{i}}= \\sum_{j=1}^{n} \\frac{\\partial Loss}{\\partial h_{0j}}\\frac{\\partial h_{0j}}{\\partial I_{i}}$$\n","\n","\n","卷积会降采样\n","\n","而回传时会升采样,BP中转置卷积=反卷积/分数步长卷积一样的\n","\n","很多有意思的项目比如小图变大图,学习到的反卷积/分数步长卷积和这个不一样, 只是形状转置\n","\n","\n","#### 卷积加速计算\n","\n","可以看出, 卷积能够转化成矩阵乘法\n","\n","把图片变成一个n列,每列都是卷积核需要经过的地方\n","\n","img2col\n","\n","把卷积核作为一个行向量\n","\n","#### 空洞卷积\n","\n","视野更大\n","\n","在语义分割有用\n","\n","以图生图\n","#### 3D卷积\n","图片的channel不算第三个维度\n","\n","真正的三维是深度或者帧数(视频)\n","\n","一个卷积核除了对图片进行遍历,也会对它深度上进行遍历\n","#### 深度卷积\n","\n","#### 可变卷积\n","\n","\n"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["### 非线性转换\n","1. 什么是非线性: 输出y不再是输入x的线性关系y=ax+b\n","2. 为了避免图像经过很多层神经网络堆叠以后还是线性的, 在每一层卷积后面加非线性变换\n","3. 什么是relu: y=max(0,x)\n","4. 为什么用relu: sigmoid导数太小,会有gradient vanishing梯度消失\n","\n","当x<0,y'=0, L对w的偏导会为0, 也就是relu死亡问题\n","\n","这样可以过拟合, 计算花销小, 对损失友好\n","\n","但是容易欠拟合\n","\n","因此可以Leaky_Relu=max(0.01x,x)\n","\n","$$\n","ELU(x)=\n","\\begin{matrix}\n","\\alpha(e^x -1), \\; x\\leq 0 \n","\\\\\n","x, \\; x>0\n","\\end{matrix}\n","$$\n","\n","除了终端层,中间层千万不要用sigmoid\n"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["### Pooling\n","1. 什么是pooling\n","    - 池化,一张图局部patch 摊开变成池子\n","    - 这些池子都是一样高的, 取值都是一样的\n","    - 有max取值方式和ave取值\n","    - 比如下图是一个kernel为2x2,stride为2,方法为max的池化,就得到了一个w/2,h/2的新图\n","    - ![5](5.png)\n","2. 为什么池化\n","    - 降采样(不是降维, 降维是通道上的)\n","    - 获取空间邻域的特征最稳定的响应情况\n","    - 可以消除噪音, 因为计算力不够\n","3. 如何BP\n","    - max: 记住原来最大的位置, 都设置成1\n","    - ave: 全都是1/4"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["### 总结\n","\n","\n","\n","传统计算机视觉是为了得到一个表示向量representative vector\n","\n","然后加入ML中加以训练\n","\n","而我们不断地卷积非线性pooling这三个操作,也是为了得到一个向量\n","\n","也就是把二维图flatten压平操作\n","\n","再连接几个FC(每次FC都要RElu)\n","\n","最终得到类别个神经元\n","\n","然后softmax得到类别\n","\n","除了Flatten,还可以用global poolng 全局池化\n","\n","这样只需要每张特征图很小,比如2x2或者4x4就行,但是数量多\n","\n","kernel=wxh stride=w=h\n","\n","一张图变成一个点, 向量中的元素数就等于特征图数\n","\n","FLatten变成GLobal pooling的原因:\n","- 太多参数,太多计算,太多资源消耗\n","- 容易过拟合\n","- gp对cnn做特征提取,而flatten损失空间信息\n","\n","\n","一开始做卷积得到很特征图,这些特征图是核的相应,\n","\n","有些核响应好\n","\n","这些核就是原图的特征,这种特征很具体,低级,像素风浓厚\n","\n","然后那些结果特征图再做卷积池化nn, 很多次迭代后\n","\n","这些特征图张数就变了,特征更高级更抽象\n","\n","比如一张猫, 一开始都是对于边缘的响应,比如 3x3\n","\n","到后面可能是耳朵,鼻子\n","\n","再往后 就是整只猫\n","\n","也就是从像素级别到语义级别\n","\n","棱锥形"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["### 其他功能层\n","\n","#### dropout层\n","训练时: FC容易过拟合,随机丢弃一些 ,不参与下一层\n","\n","测试时: 不丢,但是链接数多了,值会变大,需要再弱化,也就是根据不丢掉的概率求数学期望,\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## BN Batch Normalization\n","\n","### 0.BN的背景\n","\n","对于NN，有个炼丹的后果\n","\n","设计网络的时候，层数太多\n","\n","之间的参数传递和反传梯度更新的时候\n","\n","某次1，2层之间更新了w1，第二层的输入就变了\n","\n","梯度更新后第二层输入值的分布变了\n","\n","这样是不是要重新改变参数？\n","\n","所以难以收敛或者停止收敛\n","\n","比如Sigmoid的梯度消失\n","\n","这个BN就是针对Sigmoid\n","\n","在sigmoid的导数的两端，x趋于无穷，函数趋于0，太小了\n","\n","要使每层的输入在0附近，这样带到sigmoid的导数里不会接近0而停止更新\n","\n","如果数据分布变了，比如方差更大，更离散了\n","\n","不希望产生协变量漂移Internal Covariate Shift（由于网络参数变化引起的内部节点的数据分布变化过程）\n","\n","如何解决？做一个归一化\n","\n","\n","### 1.BN是什么\n","\n","$$y_{1}=\\vec{w_{1}}\\cdot\\vec{x}+b_{1}$$\n","$$y_{2}=\\vec{w_{2}}\\cdot\\vec{x}+b_{2}$$\n","类似的\n","$$y_{i}=\\vec{w_{i}}\\cdot\\vec{x}+b_{i}$$\n","令m为batch size\n","$$\\mu =\\frac{1}{m}\\sum_{i=1}^{m}y_{i}$$\n","$$\\sigma^2 =\\frac{1}{m}\\sum_{i=1}^{m}(y_{i}-\\mu)^2$$\n","$$\\hat{y_{i}}=\\frac{y_{i}-\\mu}{\\sqrt{\\sigma^2 + \\varepsilon }}$$\n","\\varepsilon是个很小的值\n","\n","相当于一个不知道什么分布的变成N(0,1)分布\n","\n","但是这样做，有两个弊端：\n","\n","1. 会很集中在0附近，\n","\n","sigmoid有点像线性，偏线性结果不能拟合复杂的模型\n","\n","也就是表达会对模型的复杂度损失\n","\n","------------\n","2. 比如做一个2分类学习，数据刚开始离坐标轴很远\n","\n","初始做一个归一化，相当于坐标轴移动了，这样不怎么学就分好了\n","\n","但是当层数增多时，每次卷积完，激活，因为是非线性，所以结果又偏了\n","\n","需要再归一化\n","\n","但是每层都做归一化有个问题，这样这层一学到的特征可能丢失\n","\n","需要加两个训练参数scale and shfit\n","\n","$$z_{i}=\\gamma \\hat{y_{i}}+\\beta$$\n","\n","然后再做sigmoid\n","#### 滑动平均：\n","\n","$$\\mu_{sum} = m \\times \\mu_{sum} +(1-m)\\mu$$\n","其中\\mu_{sum}是历史所有的平均\n","\n","\\mu是这次的平均值\n","\n","m一般取0.99\n","\n","这个和动量法的gd有点像，所以也用字母m表示\n","### 2. BN优势\n","\n","- 大学习率\n","- 加速训练\n","- 防止过拟合\n","- 减少L2正则\n","- 不需要dropout\n","- 更少的数据增广\n","\n","### 3. BN新看法\n","\n","ICS最近两年新看法：\n","\n","BN不能消除ICS，只是让Loss更光滑，\n","\n","局部极值点少，更容易收敛到最值\n","\n","加了BN，不管lr怎么调都很好收敛\n","\n","\n","原文是把BN放到激活函数之前\n","\n","Conv-> BN -> Sigmoid -> Pool\n","\n","后来很多人用Relu\n","\n","Conv-> BN -> Relu -> Pool\n","\n","有些人说Relu之后更好，说原作者作假\n","\n","Conv-> Relu -> BN -> Pool\n","\n","就是BN对Relu是锦上添花，对sigmoid是必须\n","\n","大致形成的说法是sigmoid，BN在前；Relu，BN在后\n","\n","为什么？这是个炼丹的过程，缺乏理论\n","\n","### BN的一些疑问\n","\n","1. BN里面求的hat_{x}, 和batch size有关，是不是size越大，越好？\n","\n","在训练时\n","\n","受限于计算机的存储计算能力，并不能取太大\n","\n","然后对于\\mu \\sigma,每次都要记录做均值\n","\n","以便，在测试时，只有单张时，使用\n","\n","2. Batch size 很小会有变化：\n","\n","### TODO LN IN GN\n","\n","### TODO 到底是x还是y"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["## 总结 CNN的基本要素 也就是Layers和Modules\n","### Fundamental Layers\n","Conv, Dilated Conv, Transposed Conv, C3D(Deformatble Conv)\n","\n","ReLU, Leaky_Relu,PRelu,Elu(Selu)\n","\n","Pooling:max, ave\n","\n","FC\n","### Functional Layers\n","Drop\n","\n","BN\n"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}