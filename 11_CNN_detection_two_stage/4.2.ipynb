{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitdeeplearningconda4eb25749ed314365ad1de507c177859b",
   "display_name": "Python 3.7.6 64-bit ('deeplearning': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类\n",
    "分类问题主要是前景的物体\n",
    "\n",
    "背景不关心\n",
    "\n",
    "我们希望前景多,背景少\n",
    "\n",
    "也就是说分类问题应用比较少\n",
    "\n",
    "## 检测 Detection\n",
    "\n",
    "### 不仅分类classification \n",
    "\n",
    "需要  n + 1\n",
    "\n",
    "这个1是背景\n",
    "\n",
    "### 还要定位location \n",
    "\n",
    "框框 叫 bonding box\n",
    "\n",
    "需要位置(中心点x,y,长,宽)或者(左上xy,右下xy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两阶段检测\n",
    "\n",
    "比较符合人的思维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 检测问题常见数据集\n",
    "> 检测问题是监督学习,  需要数据集\n",
    "\n",
    "常见分类数据集:\n",
    "- ImageNet\n",
    "- Mnist\n",
    "- CIFAR 10/100\n",
    "\n",
    "检测数据集:\n",
    "- Imagenet也有, 质量不好, 但是不常用\n",
    "\n",
    "- Pascal VOC \n",
    "    - 两个版本2007, 2012\n",
    "    - 2005年公布, 从07年开始一年一次竞赛, 评分标准mAP, 12年竞赛截止\n",
    "    - pattern analysis statistical modeling and computational learning visual object classes\n",
    "    - 大概10k张图片, 20个类别, 每个图2-3个obj, 共20k-30kobj\n",
    "\n",
    "- COCO\n",
    "    - 微软\n",
    "    - 200k张图, 80类, 500-600k 大小20g\n",
    "    - 小目标, 不中心对称, 重叠, 比较难\n",
    "    - common objects in context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 传统机器学习如何检测\n",
    "1. Batch x Channel x Height x Width的图片张量\n",
    "2. 提取特征, 比如Haris, SIFT, HOG\n",
    "3. 特征进一步筛选, 聚类, 比如RANSAC\n",
    "4. 得到一些向量, 输入到SVM, NN, 决策树等机器学习工具\n",
    "5. 两个任务, 分类, 坐标回归(定位)\n",
    "\n",
    "每一步都要人工干预, 都很关键, 前面错了后面就白做了\n",
    "\n",
    "![1](1.png)\n",
    "结果\n",
    "- TP: 正确框出物体\n",
    "- FP: 图中确实有某个物体,框错了\n",
    "- FN: 本来是背景, 但是标错了. 相当于把物体标成了背景, 但是背景一般是没有框的, 也就是一个物体没画框, 相当于漏检\n",
    "- TN: 正确框了背景, 背景不用标其实"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN做检测\n",
    "## 思路\n",
    "应该先定位, 大化小, 多化少, 再分类\n",
    "\n",
    "关键是定位怎么做呢\n",
    "\n",
    "- 前景分隔\n",
    "\n",
    "## 2014第一次尝试 R-CNN\n",
    "![2](2.png)\n",
    "\n",
    "把特征向量的提取由传统算法换成cnn\n",
    "\n",
    "1. 区域建议 Region Proposal\n",
    "- Selective Search算法(很复杂,很难用)\n",
    "    - 把图片中纹理, 颜色相近的区域相连接\n",
    "    - 区块进行区域建议\n",
    "- 生成大约2000个区域\n",
    "    - 为了保证需要的都在里面, 不得已,必须这么多\n",
    "- 每一个都要做resize, 把图片尺寸固定一个大小(因为CNN中FC要求图片参数固定)\n",
    "    - 弊端: 蛇, 长杆容易改变特征\n",
    "- 进入二次定位的分类网络, 也就是下一步了\n",
    "2. 用cnn提取特征向量\n",
    "    - 这里是AlexNet, **只拿了特征提取层**\n",
    "    - 每张图片一张一张送入 \n",
    "    - 后面多加了几层,比如FC, Softmax\n",
    "    - 先做了pretrain, 然后finetune\n",
    "    - 用真实数据(图片和bonding box做iou计算得到真实数据)进行finetune, 主要更新后面的层的参数\n",
    "        > iou是intersection over union, 表征重叠程度, 程度高越认为是前景\n",
    "        > iou是交并比\n",
    "        >\n",
    "        > 图中红框是真实的ground truth\n",
    "        >\n",
    "        > 蓝框是区域建议出来的\n",
    "        >\n",
    "        > 交集绿色/总和, 也就是: 绿/(红+蓝-绿)\n",
    "        >\n",
    "        > ![3](3.png)\n",
    "        > \n",
    "        > 一个经验化的标准, >0.7的正样本, <0.3 认为是负样本\n",
    "        > \n",
    "    - 此时batch=128=32有物体+96没物体的(1:3是经验)\n",
    "3. 分类(SVM) + 2次定位(线性模型+正则化项)\n",
    "    - 有softmax还要再设置svm, 这是因为softmax很差, 把softmax丢掉用svm好很多\n",
    "        - 因为正样本就是原始的正值(ground truth), 而负样本是<iou的, 但是这里负样本的iou比cnn的负样本的iou小, 数量更少, 质量更高(CNN那边不得已而为之,保证数据够多够用, 否则,数据不够还那么复杂网络, 会过拟合)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. NMS算法 Non-Maximum Suppression\n",
    "\n",
    "> 2014的这个还用了NMS算法\n",
    ">\n",
    "> 至今还在用\n",
    ">\n",
    "> 这是个精简bonding box个数的算法\n",
    "\n",
    "原则:\n",
    "\n",
    "1. 每个框Score越高, 越想要\n",
    "2. 框与框的iou越大,这两个框越可能是一个物体\n",
    "\n",
    "也就是选出Score高的框, 然后丢掉和它iou高的其他框\n",
    "\n",
    "这个阈值越低越严格\n",
    "\n",
    "Input: \n",
    "- score_list: \\[score_0, score_1, ..., score_n\\]\n",
    "- bbox_list: \\[bbox_0, bbox_1, ..., bbox_n \\]\n",
    "- threshold\n",
    "\n",
    "Process:\n",
    "\n",
    "- 找到bbox_list中score_list最大的bbox_m, bbox_list删掉它, 并把它放到new_list中\n",
    "    - 遍历bbox_list, 每个bbox_i和刚才的bbox_m做iou, 大于threshold, bbox_list就删掉bbox_i(相应的score_list也要删掉对应的)\n",
    "- 直到bbox_list为空\n",
    "- 返回new_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 传统NMS弊端与改进\n",
    "\n",
    "![10](10.png)\n",
    "\n",
    "- 这两个框互斥, 某个score高另一个就可能不会被选中\n",
    "- NMS只能用CPU\n",
    "- NMS依赖阈值, 阈值是个很经验的东西, 大了容易漏检, 小了容易误检"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft NMS\n",
    "#### 传统的NMS:\n",
    ">  大于threshold, bbox_list就删掉(相应的score_list也要删掉对应的)\n",
    "\n",
    "这里score_list的删除和保留操作, 相当于:\n",
    "$$\n",
    "\\begin{align*}\n",
    " &S_{i}=\n",
    " \\left\\{\\begin{matrix}\n",
    " S_{i}&if\\; iou(bbox_{m},bbox_{i})<threshold \\\\ \n",
    "0 & if\\; iou(bbox_{m},bbox_{i})\\geqslant threshold\n",
    "\\end{matrix}\\right. \n",
    "\\\\ \n",
    " &= \n",
    " S_{i}\\left\\{\\begin{matrix}\n",
    " 1 &if\\; iou(bbox_{m},bbox_{i})<threshold \\\\ \n",
    "0 & if\\; iou(bbox_{m},bbox_{i})\\geqslant threshold\n",
    "\\end{matrix}\\right.\n",
    "\\end{align*}\n",
    "\n",
    "$$\n",
    "\n",
    "也就是传统是\"硬\"的, 粗暴的0和1\n",
    "\n",
    "#### Soft NMS不做删除:\n",
    "$$\n",
    "S_{i}=S_{i} \\cdot \\left\\{\\begin{matrix}\n",
    " 1&if\\; iou(bbox_{m},bbox_{i})<threshold \\\\ \n",
    " 1-iou & if\\; iou(bbox_{m},bbox_{i})\\geqslant threshold\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "可以看出1-iou是个关于iou的递减函数\n",
    "\n",
    "所以我们甚至可以写成一个式子, 并且摆脱阈值: 其中sigma 很小\n",
    "\n",
    "$$ S_{i}=S_{i} \\cdot exp(- \\frac{iou^2}{\\sigma})$$\n",
    "Input: \n",
    "- score_list: \\[score_0, score_1, ..., score_n\\]\n",
    "- bbox_list: \\[bbox_0, bbox_1, ..., bbox_n \\]\n",
    "- threshold\n",
    "\n",
    "Process:\n",
    "\n",
    "- 找到bbox_list中score_list最大的bbox_m, bbox_list删掉它, 并把它放到new_list中\n",
    "- 遍历bbox_list, 每个bbox_i和刚才的bbox_m做iou, score_list对应的就要变小, 变小的系数和iou有关\n",
    "\n",
    "#### 注意: \n",
    "- 之后可能需要筛选score的阈值\n",
    "- 训练时不用soft, 测试时用更好, 说明易用性强"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Bonding Box Regression\n",
    "对偏移量做回归\n",
    "\n",
    "归一化\n",
    "\n",
    "参看[Faster RCNN](#Faster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2014 - RCNN 缺点&问题:\n",
    "\n",
    "\n",
    "- 2000次输入CNN卷积, 太慢(大概一分钟一张),  重合的地方没必要重复卷积\n",
    "- 用SVM不好\n",
    "- 计算复杂, 结构臃肿, 一会儿用cnn, 一会儿丢弃softmax, 一会儿用SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改进 - 2015 - Fast RCNN \n",
    "\n",
    "> 每秒1-2张\n",
    "\n",
    "![11](11.png)\n",
    "\n",
    "1.  原本selective search生成2000个区域, 每个去卷积\n",
    "\n",
    "    现在原图还是selective search生成2000个区域, 投影到特征图上(RoI projection. region of interest), 特征图做一次卷积\n",
    "\n",
    "    本质上也就是对特征图分割了, 而不是原图\n",
    "\n",
    "    投影时, 要注意卷积网络的pooling, 将尺寸变小了, 所以投影也要把尺寸相应变小, 比如vgg16, 就需要/16\n",
    "    \n",
    "    \n",
    "2.  投影到特征图上的小区域还是不一样小的, 需要变成一样小, \n",
    "    \n",
    "    resize不行, 长条会失去形状\n",
    "    \n",
    "    用RoI pooling layer(RoI pooling仍然可以改进)\n",
    "\n",
    "3.  SVM -> Softmax\n",
    "\n",
    "4.  训练的时候, IoU>0.5的是正样本, IoU在0.1-0.5的是负, 0-0.1的用于验证. \n",
    "\n",
    "    验证错误的例子重新训练. 这叫Hard Example Mining\n",
    "\n",
    "5.  两个分支输出, 因此有多个loss. \n",
    "\n",
    "    位置,坐标回归Loss用L1,以避免L2大的更大小的更小; \n",
    "    \n",
    "    分类的Loss用交叉熵\n",
    "\n",
    "    注意, 背景没有位置的loss, 所以可以用01系数来算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI Pooling 讨论\n",
    "\n",
    "投影之后需要同样缩小尺寸, 如前文的/16\n",
    "\n",
    "可能会取整\n",
    "\n",
    "然后变成尺寸一样大的, 这里需要用网格划分\n",
    "\n",
    "每个网格做max pooling方面\n",
    "\n",
    "![12](12.png)\n",
    "\n",
    "但是, 比如5x7的变成2x2的, 也会取整\n",
    "\n",
    "取整会丢失信息, 尤其是小的区域\n",
    "\n",
    "### 如何改进\n",
    "1. 投射方面\n",
    "\n",
    "2. RoI pooling方面\n",
    "\n",
    "\n",
    "### RoI align\n",
    "![13](13.png)\n",
    "\n",
    "投射的时候/16, 得到小数点, 没关系, 保留浮点数\n",
    "\n",
    "网格划分, N=4(经验), 每个小网格(bin)内取N块\n",
    "\n",
    "bin的max值就是N个块的max的max\n",
    "\n",
    "取每个小块的正中间的点,代表这个块\n",
    "\n",
    "这个点的坐标一般是浮点数了\n",
    "\n",
    "这个点显然附近有4个坐标为整数的像素点(确切来说叫特征点)\n",
    "\n",
    "这个点的值,用4个点的双线性插值\n",
    "\n",
    "$$ $$\n",
    "### 缺点\n",
    "\n",
    "N超参数,要提前指定\n",
    "\n",
    "并不是所有点贡献进来了, 并不一定是每个块的中心点附近有最值\n",
    "\n",
    "### precise roi (2018-iou-net)\n",
    "也是两次都保留浮点\n",
    "\n",
    "对于bin, 其实存在虚点, 和原来的特征点有位置偏差\n",
    "\n",
    "用双线性插值, 根据周围每一个虚点4个实点, 计算每一个的虚点\n",
    "\n",
    "看起来计算量大,但是用到了很多重复的点, 算法可以优化的\n",
    "\n",
    "算完之后求average pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id='Faster'></span>\n",
    "## 进一步改进 Faster RCNN 2015\n",
    "\n",
    "[论文地址](https://arxiv.org/pdf/1506.01497.pdf)\n",
    "\n",
    "> 里程碑, 很重要\n",
    ">\n",
    "> 每秒10帧\n",
    ">\n",
    "> 起初为了改变selective search 的效率低下问题\n",
    ">\n",
    "> 思考: 能不能用CNN代替那些复杂的数学计算呢?\n",
    "\n",
    "### 结构概述\n",
    "![15](15.png)\n",
    "\n",
    "1. backbone 用CNN将image变成feature map\n",
    "2. RPN, 将feature map变成region proposal或者RoI\n",
    "3. RoI pooling 然后用 Fast RCNN得出回归和分类的loss, 迭代训练\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backbone + rpn 的测试阶段:\n",
    "![30](30.png)\n",
    "\n",
    "1.  用backbone得到feature map, 比如是WxHxC\n",
    "\n",
    "2.  先用cx3x3xn的卷积, 得到WxHxn. 这一步是语义转换, 增加rpn的拟合能力, n一般512\n",
    "\n",
    "3.  然后分支\n",
    "\n",
    "    18= 9x2 分类信息\n",
    "\n",
    "    36= 9x4 坐标回归信息\n",
    "\n",
    "    这个2和4固定的\n",
    "\n",
    "    9是9个anchor值\n",
    "\n",
    "    anchor 对应原图某个位置,  这个位置可能有2个分类概率和4个坐标回归\n",
    "\n",
    "3.1 nx1x1x18的卷积, 得到WxHx18\n",
    "\n",
    "    18的先reshape成 2xWxHx9 做softmax, 再reshape回去\n",
    "\n",
    "    这个reshape是原文中caffe特有, torch可以不用\n",
    "\n",
    "    2是因为2分类(是物体还是不是物体)\n",
    "\n",
    "3.2 另一个分支 nx1x1x36的卷积\n",
    "\n",
    "4.  合并 , 得到region proposals, 这里我们只关心这个是前景还是背景\n",
    "\n",
    "> 两阶段的含义: 1找出物体,2找出物体是什么类别\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### anchor\n",
    "\n",
    "\n",
    "现在每个特征点对应原图9个框\n",
    "\n",
    "3种w和h比例:\n",
    "- 1:1\n",
    "- 2:1\n",
    "- 1:2\n",
    "\n",
    "3种特征点大小和原图比例:\n",
    "- 16\n",
    "- 256\n",
    "- 512\n",
    "\n",
    "组合一下就是9个\n",
    "\n",
    "例如 , 如果特征图是38x50 的\n",
    "\n",
    "那么就有9x38x50=17100个anchor\n",
    "\n",
    "具体情况下, 根据训练集的label, 可以更改那些比例\n",
    "\n",
    "比如蛇, 10:1, 1:10 , 1:20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  RPN训练阶段\n",
    "![40](40.png)\n",
    "\n",
    "用特征图创建原图的anchor, 叫做create anchor, 比如17100个\n",
    "\n",
    "拿出ground truth(这个里面只有框)\n",
    "\n",
    "anchor和gt做iou和nms, 得到9x2和9x4, 相当于特征点的anchor_label\n",
    "\n",
    "和rpn中得到的9x2, 9x4分别做cls_loss, loca_loss\n",
    "\n",
    "这样就可以训练rpn\n",
    "\n",
    "$$Loss = cls\\_loss + \\lambda loca\\;\\_loss$$\n",
    "\n",
    "\\lambda 常取0.1\n",
    "\n",
    "#### loca_loss 之 smooth L1 loss\n",
    "loca_loss使用, 当然只有类别是物体才算这个loss\n",
    "\n",
    "$$\n",
    "f(x)=\\left\\{\\begin{matrix}\n",
    "\\frac{(\\sigma x)^2}{2} &  if \\; x<\\frac{1}{\\sigma^2}\\\\\n",
    "|x|-\\frac{1}{2 \\sigma^{2}} & otherwise\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "#### Bonding Box Regression\n",
    "\n",
    "bbox两种数据结构:\n",
    "1. 中心点(x, y)和w,h\n",
    "2. x1, y1, x2, y2\n",
    "\n",
    "在L1 中, 要预测偏移量, 也要对大小物体要归一化\n",
    "\n",
    "\n",
    "\n",
    "![51](51.png)\n",
    "![50](50.png)\n",
    "\n",
    "create anchor 生成的是a\n",
    "\n",
    "走模型预测出来的是p\n",
    "\n",
    "我们本来想训练这个网络如何从a到t\n",
    "\n",
    "但其实是p->t\n",
    "\n",
    "迁移预测 p-a 去接近t-a\n",
    "\n",
    "以第一种为例: \n",
    "\n",
    "$$x_{p-a}=\\frac{x-x_a}{w_a}$$\n",
    "\n",
    "$$y_{p-a}=\\frac{y-y_a}{h_a}$$\n",
    "\n",
    "$$w_{p-a}=\\log \\frac{w}{w_a}$$\n",
    "\n",
    "$$h_{p-a}=\\log \\frac{h}{h_a}$$\n",
    "\n",
    "$$x_{t-a}=\\frac{x^*-x_a}{w_a}$$\n",
    "\n",
    "$$y_{t-a}=\\frac{y^*-y_a}{h_a}$$\n",
    "\n",
    "$$w_{t-a}=\\log \\frac{w^*}{w_a}$$\n",
    "\n",
    "$$h_{t-a}=\\log \\frac{h^*}{h_a}$$\n",
    "\n",
    "最终, 对于某一个物体:\n",
    "\n",
    "$$loca\\_loss = \\frac{1}{4}\\sum_{i\\in\\{x,y,w,h\\}}^{4} f(i_{p-a}-i_{t-a})$$\n",
    "\n",
    "#### cls_loss\n",
    "\n",
    "> 9x4解决了, 那么9x2怎么解决呢?\n",
    "\n",
    "很多anchor比如17100个, 过边界筛除, 太小筛除, 1w左右\n",
    "\n",
    "NMS筛除\n",
    "\n",
    "然后IoU筛除, >0.7正样本, <0.3负样本标签\n",
    "\n",
    "得到128个框,含有物体: 不含物体=1:3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Rcnn阶段\n",
    "\n",
    "rpn中得到9x2和9x4后, 要和feature map结合\n",
    "\n",
    "做roi pooling\n",
    "\n",
    "输入到fast rcnn中\n",
    "\n",
    "(其实这里相当于拿到了region proposal, 并且是比selective search牛逼的region proposal)\n",
    "\n",
    "\n",
    "让这些小的特征图再进行回归和分类(20)\n",
    "\n",
    "![60](60.png)\n",
    "\n",
    "#### 存在问题:\n",
    "\n",
    "训练rpn的时候 backbone变了\n",
    "\n",
    "fast rcnn 也包含这个backbone, 同时用到了rpn的输出\n",
    "\n",
    "所以, 用rpn的数据来训练fast rcnn, backbone又变了\n",
    "\n",
    "#### 联合训练alternative training:\n",
    "\n",
    "1. 先训练rpn, 包括backbone和后面那些512, 18, 36等特有部分\n",
    "2. 训练rcnn ,此时会改变backbone\n",
    "3. 使用2中的backbone, 重新训练rpn, 此时保持backbone不变, 只改变特有部门\n",
    "4. 重新训练rcnn,固定整个rpn都不变\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### faster rcnn GitHub复现\n",
    "\n",
    "[最简单的用torch复现的,作者: chenyuntc](github.com/chenyuntc/simple-faster-rcnn-pytorch)\n",
    "\n",
    "[腾讯open-mmlab, 图像检测工具包](github.com/open-mmlab/)\n",
    "\n",
    "[detectron2, 挺新的](github.com/facebookresearch/detectron2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "怎么由gt和create anchor得到9x4和9x2的?\n",
    "\n",
    "这是针对每一个特征点的吗?\n",
    "\n",
    "predict中如何产生9x4和9x2的?\n",
    "\n",
    "128又是什么?"
   ]
  }
 ]
}