{"cells":[{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["# CNN的网络结构 "]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","## Outline \n","### 历史上的经典结构类型\n","- 开山(已淘汰)\n","    - 史前\n","    - 1998 LeNet\n","        - 只有5层, 也叫LeNet5\n","    - 2012 AlexNet\n","        - 在历史的发展中很重要, 为其他框架提供了借鉴意义\n","        - 最早开始有CAFFE框架\n","        - GPU普及, 算力提升\n","        - 转折点, DP开始火爆\n","    - 2013 ZFNet\n","        - 可视化分析\n","- 二代(仍用)\n","    - 2014 VGG(Visual Geometry Group) LeNet\n","        - 很好的特征提取网络\n","        - 比较深\n","    - 2014 GoogLeNet(inception)\n","        - 模块化, 积木化\n","- 三代\n","    - 2015 ResNet\n","        - 更深(成百上千)\n","    - 模型的轻量化\n","        - SqueezeNet->MobileNet -> ShuffleNet\n","    - 2017 DenseNet\n","    - 2017 CapsNet\n","    \n","### 模型的参数\n","- 参数多少: FLOPs, 能衡量模型的大小,\n","- 计算次数: MACs计算次数\n","\n","### 模型的结构\n","- 模型都是由backbone(提取特征,加以组合, 也就是backbone就是特征处理部分)得到feature map,\n","- 然后功能层(或是分类,或是检测)对fm做处理得结果\n","\n","### 分类模型正确率的提高\n","- 2011 XRCE 74%\n","- 2012 AlexNet 83.6%\n","- 2013 ZFNet 88.3%\n","- 2014 VGG 92.7%\n","- 2014 GoogLeNet 93.3%\n","- **human** 95%\n","- 2015 ResNet 96.4%\n","- 2016 GoogLeNet - v4 96.9%"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["## 1998 LeNet\n","\n","3@28x28 -> Conv 5x5x3x20 -> tanh -> 20@24x24 -> max-pooling 2x2 -> 20@12x12 -> Conv 5x5x20x50 -> tanh -> 50@8x8 -> max-pooling 2x2 -> 50@4x4 -> Conv -> tanh -> Flatten -> 500 -> FC ->tanh\n"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["## AlexNet\n","3@224x224 -> Conv 11x11x3x96 stride = 4 -> 55x55x96 ->  Conv 5x5x96x256 stride = 1 -> max-pooling 2x2 -> LRN -> 27x27x256 -> Conv 3x3x256x384 -> max-pooling 2x2 -> LRN -> 13x13x384 -> Conv 3x3x384x384 -> 13x13x384 -> Conv 3x3x384x256 -> 13x13x256 -> max-pooling -> Flatten -> FC\n","\n","- 用了Relu\n","- 用了Dropout\n","- 用了重叠的max pooling\n","    - 比如3x3的窗口,每2个遍历一次, 这样就会有重叠的\n","- 用了多个GPU计算\n","- LRN Local Response Normalization技术（一种归一化技术，已被淘汰）"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["## ZFNet\n","\n"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["## 小结\n","\n","在这一历史时期，这一经验公式empirical formula为核心：\n","\n","Input -> n x (Conv - ReLU - MaxPooling) -> 1|2 x FC -> Output\n","\n","刚起步阶段，其中很多东西都可以改进\n"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["## VGG\n","- 卷积核进一步缩小\n","- 连续卷积再pooling，而不是卷积pooling\n","- pooling 2x2 stride =2 \n","- 通道很多\n","- 一般是16层/19层\n","- 只用3x3的卷积核\n","    - 最早倾向去前面的layer用大核，后面用小核，大核的感受野大，提取的特征区域也大\n","    - 而小核用多次，也能实现更大区域的特征。例如两次3x3就是一次5x5的，三次3x3就是一次7x7的\n","    - 三次3x3需要27x原图channel数个参数，而一次7x7需要49x原图channel数个参数，节省空间\n","    - 小核和大核有一样的RF加速算法，小核对Winograd加速算法友好\n","    - 注意channel会变化，比如channel比较少，大核也可以"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["## GoogLeNet\n","\n","- 启发于盗梦空间，很深，很宽\n","- 其中一个模块重复了很多次\n","    - 前一层有四个分支，分别做卷积，然后做depthConcat，也就是图像大小一样，张数叠加\n","    - 用这四个分支，获得感受野的区域不一样，我们把这四个不同感受野放在一起，特征更加丰富\n","- 3个输出\n","- 没有FC(初代有)\n","- Global ave pooling\n","    - 比如500个特征图, 每个图7x7, 在7x7中取一个平均值,得到500个均值\n","    - 减少了Fc操作\n","- auxiliary softmax 辅助的\n","    - 有三个输出, 每个输出都和真实值做loss, 然后梯度回传\n","    - 三个模型的输出做一个ensemble, 也就是对loss有权值, 加权求和一起回传\n","    - 层数浅的, 梯度可以大,权重小,  收敛快\n","    - 减少过拟合\n","- 网络层数增加,参数也增加, 增强你和能力\n","- 借鉴了Hebbin's Principle, 多个神经元多次同时激活,  某次只激活一个,其他的也会按惯例激活"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["## ResNet\n","###  背景\n","\n","归根结底,网络不够深, 不能拟合复杂的\n","\n","设计的很深的时候, 比如56层对比5层, 和20层对比5层, 随着iteration增加, 虽然loss都降低\n","\n","20层的确比5层的loss小,但是50的loss比20的大\n","\n","每次回传的参数变化很小\n","\n","梯度消失, 梯度下降法不太有效了\n","\n","即便是relu,也只是缓解了梯度消失,层数多的时候还是会梯度消失\n","\n","### 过程特点\n","- pooling次数很少\n","- 每个模块, 两个分支,一个做多层卷积,一个直接结果, 然后对应元素相加(size要一样),再激活\n","    - H(x) = F(x) + x这样求导不会小于1, 保证了梯度必定回传\n","    - F(x) = H(x) - x标签和输入的差,也叫残差,所以也叫残差网络\n","- bottle neck先降维再升维(瓶颈形)\n","- 一开始第一层7x7\n","    - 靠前的layer, channel 少的时候用大核, 尽量多地提取特征\n","    - ![10](10.png)\n","    - ![11](11.png)\n","- 虚线的地方跳阶,对应不跳阶的地方stride=2,因此\"直接结果\"的分支上需要加一个卷积;实线跳阶的地方对应stride=1\n","\n","### 改进\n","- 更深\n","    - [Identity Mappings in Deep Residual Networks](https://arxiv.org/pdf/1603.05027.pdf)\n","- 与Inception结合,32个分支, 宽度加宽.\n","    - [ResNext](https://arxiv.org/pdf/1611.05431.pdf)\n","\n","### 为什么ResNet更好\n","多模型融合的优势, 就像电阻串并联, 可以排列组合出很多情况\n","\n","![15](15.png)\n"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["## DenseNet\n","由ResNet发展而来, 的确更好,但是太花费开销了\n","\n","神经元任意两个都要连接, 所有的组合"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["# 更轻量化的框架\n","2015 - 2016 backbone已经到头了, 模型的深度和宽度也没什么了\n","\n","之后发展的趋势就是轻量化\n","\n","比如华为2020 ghost net\n","\n","两个问题:\n","- 模型的参数\n","- 计算的次数(加法 乘法)\n","\n","解决:\n","- 模型本身\n","    - 剪枝, 量化(浮点变整型)\n","    - **结构**\n","        - 优化网络结构: ShuffleNet\n","        - 单纯减少参数: SqueezeNet\n","        - 优化卷积: 深度分离卷积, 组卷积\n","        - 删除FC, 用global pooling代替\n","- 外在方面\n","    - 设备(移动端/嵌入式)\n","    - 降低准度要求\n"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["## SqueezNet\n","\n","- 来源于GoogLeNet\n","- 少用3x3,多用1x1,用3x3的时候channel少一点\n","- 为了防止参数太少, 晚点用pooling\n","\n","### 结构过程\n","Fire Module(替代GoogLeNet中的那个块):\n","\n","1. SqueezNet\n","    - 1x1 Conv 降维 成S1个通道\n","2. Expand\n","    - 两个分支 E1个1x1和E3个3x3\n","    - 深度concat\n","\n","要求: \n","- S1/(E1+E3) =0.75比较好\n","- E3/(E1+E3) 从0.5开始调\n","\n","3种模型,第二种比较好:\n","\n","![20](20.png)"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["## MobileNet V1\n","### 深度分离卷积:\n","\n","1.Depthwise Convolution:\n","\n","HxW的图,C个通道, 有C个KxKx1的卷积核, 一一对应做卷积(而不是全部)， 不叠加，还是得到C个结果\n","\n","2.然后有N个1x1xC个卷积核,对上一步结果做普通卷积（c个通道对应卷积再叠加，重复N次）得到N个图\n","\n","实质上相当于对于原来的KxKxCxN的卷积分开了，\n","\n","#### 参数个数比较:\n","\n","不考虑padding和stride\n","\n","原来的方式:KxKxCxNxHxW=N·K^2·HWC 次遍历操作\n","\n","深度分离卷积: KxKx1xCxHxW + 1x1xCxNxHxW = (N+ K^2) HWC 次操作\n","\n","显然小了很多\n","### 过程与结构\n","\n","![p23](23.png)\n","\n","- 不能直接把深度分离卷积换成原来的, 还需要做一些修补\n","\n","    - 原来的： 3x3 Conv -> BN -> ReLU\n","\n","    - 现在：3x3 Depthwise Conv -> BN -> ReLU -> 1x1 Conv -> BN -> ReLU\n","\n","- 全局ave pooling\n","\n","\n","- HyperParameter 超参数，也叫宽度因子alpha来控制瘦身程度， 分辨率因子\\rho 控制特征图尺寸\n","\n","$$K\\times K\\times 1\\times C\\times H\\times W + 1\\times 1\\times C\\times N\\times H\\times W $$\n","$$K\\times K\\times 1\\times\\alpha C\\times\\rho H\\times \\rho W + 1\\times 1\\times\\alpha C\\times\\alpha N\\times\\rho H\\times\\rho W $$\n","$$\\alpha \\in (0,1],\\;\\;\\rho \\in (0,1]$$\n","\n","\n","Multiply-Accumulates乘法加法累积总数，简称MAC\n","![24](24.png)\n","\n","### 一些细节\n","- 不太用于过拟合\n","- 不用太多数据增广\n","- 取消了L2正则化"]},{"cell_type":"markdown","metadata":{},"source":["## MoblieNet V2\n","两个改进：（借鉴于ResNet）\n","- ### Inverted Residuals \n","    Skip connection这种瓶颈结构很有效\n","\n","    但是参数再压缩就没了\n","\n","    和ResNet类似，也是两个分支,一个做多层卷积,一个直接结果, 然后对应元素相加\n","\n","    但是这个多层卷积不是channel先变小在变大，而是先变大在变小\n","\n","    比如 让\\alpha >1\n","    \n","    具体来说就是先扩大expansion convolution， 再做depthwise convolution，然后再缩小reduction convolution\n","\n","\n","- ### Linear BottleNecks\n","    在那里不需要relu，relu可能损失小于0的神经元\n","\n","    本来就参数不多，学不到东西\n","\n","此外，和ResNet不同的是，它有一种不跳阶的块：1x1Conv 然后Depthwise Conv 3x3 stride=2"]},{"cell_type":"markdown","metadata":{},"source":["## MobileNet V3\n","借鉴SENet，分支上用了一个全局的pooling\n","\n","swish做激活函数\n","\n","relu 6（到6截至）放在后面\n"]},{"cell_type":"markdown","metadata":{},"source":["## ShuffleNet V1\n","### 组卷积\n","\n","对输入W x H x C的channel数分成g组， 对于每一组，有只供这一组用的k x k x C/g x N的卷积核，对这一组做N次普通卷积，（也就是C/g个平面卷积核对输入卷积再叠加，重复N次），得到N个输出，总共所有的组得到g x N个输出。\n","\n","把每个组看成一个整体，这就像是深度卷积\n","\n","分1组，就是传统卷积（相当于一个大核对所有的输入图进行卷积）\n","\n","分组数和输入数等量：就是Depthwise Convolution\n","\n","运算次数分析：用k x k x C/g x N x H x W得到一组的输出，要得到所有组的输出，k x k x C/g x N x H x W x g，也就是k x k x C x N x H x W，但是它的输出数量是普通卷积的g倍，所以相当于运算次数是普通卷积的1/g\n","\n","这里的kernel一般是1x1的，为了通道可以叠加\n","\n","得到的gxN的结果，通道和卷积核的相关性太高了，模型拟合能力差\n","\n","需要加一个shuffle\n","\n","![30](30.png)"]},{"cell_type":"markdown","metadata":{},"source":["## ShuffleNet V2\n","Gudie lines:\n","\n","1. 内存访问消耗MAC（memory access cost）最小，当且仅当输入通道数=输出通道数\n","2. 组卷积会增加MAC\n","3. 分支增加也会增加MAC，降低平行度\n","4. 虽然卷积占运算的大头，对应元素的操作是不可以忽略的"]},{"cell_type":"markdown","metadata":{},"source":["## 轻量化框架对比\n","速度：\n","\n","ShuffleNet-v2 > MolibeNet-V2 > ShuffleNet > MolibeNet > SqueezeNet\n","\n","优先选ShuffleNet和MobileNet-v2\n","\n","其他：\n","Xception resNexXt MobileID GhostNet\n","\n","## 其他框架\n","feature pyramid network 金字塔\n","\n","hourglass 沙漏模型 以图生图\n","\n","Siamese 暹罗猫（孪生）网络，用于少样本的"]},{"cell_type":"markdown","metadata":{},"source":["## "]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["## 杂记\n","IMAGENET 有1400万张图片分类, 1000类\n","\n","100万张用于检测, 定位, 框出来, 分类\n","\n","2012年有Imagenet Large Scale Visaul Recognition Competition, AlexNet第一名\n","\n","2014 ,这一比赛GoogLeNet冠军\n","\n","2015 , ResNet冠军\n","\n","resize 有很多方式， crop， 插值， 补充\n","### TODO 3通道变成N通道, 需要kxkx3xN的kernel, 中间层N个通道了, 然后再conv, 得到N'个通道需要多少个kernel?kxkxNxN'?\n","### 解答: 是的\n","\n","### TODO VGG为什么pooling后能够size变小而channel变多？\n","### 解答： 因为做了卷积+stride操作\n","卷积核影响模型的大小（参数个数）和拟合效果\n","\n","model size 只体现在参数上\n","\n","FC是head，backbone是特征部分，而fc拉伸之后失去特征含义，属于功能层\n","\n","### 实际应用\n","\n","性能上来看，要变得更深\n","\n","不考虑设备问题，先用Vgg或者Resnet做特征提取\n","\n","再加功能层"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}